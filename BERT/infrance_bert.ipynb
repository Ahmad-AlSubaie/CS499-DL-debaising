{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cb4espuLKJiA"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Hub Authors.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "id": "jM3hCI1UUzar"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZ6SNYq_tVVC"
      },
      "source": [
        "# Classify text with BERT\n",
        "\n",
        "This fine-tuned BERT model is trained to classify tweets into 3 labels, 0 = \"hate speech\", 1 = \"offensive language\", 2 = \"neither\". \n",
        "\n",
        "Trained on https://github.com/t-davidson/hate-speech-and-offensive-language\n",
        "\n",
        "testing dataset from https://github.com/hate-alert/HateXplain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCjmX4zTCkRK"
      },
      "source": [
        "## Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "q-YbjCkzw0yU"
      },
      "outputs": [],
      "source": [
        "# A dependency of the preprocessing for BERT inputs\n",
        "#!pip install -q -U \"tensorflow-text==2.8.*\"\n",
        "#!pip install -q tf-models-official==2.7.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loading in packages and initilizing constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_XgTpm9ZxoN9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From C:\\Users\\Ahmad\\AppData\\Local\\Temp/ipykernel_55952/554798616.py:19: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.config.list_physical_devices('GPU')` instead.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "from official.nlp import optimization  # to create AdamW optimizer\n",
        "\n",
        "\n",
        "from datasets import load_dataset\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = '/GPU:0' if tf.test.is_gpu_available()else \"/CPU:0\"\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "MODEL_NAME = \"hate_bert\"\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "batch_size = 12\n",
        "seed = 42\n",
        "test_train_ratio = 0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6MugfEgDRpY"
      },
      "source": [
        "## Loading in datasets\n",
        "\n",
        "hate_data holds the train and validaiton dataset from the hatespeech dataset\n",
        "\n",
        "train_ds, val_ds, and test_ds are batched and shuffled tf.data.Dataset "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Reusing dataset hatexplain (C:\\Users\\Ahmad\\.cache\\huggingface\\datasets\\hatexplain\\plain_text\\1.0.0\\df474d8d8667d89ef30649bf66e9c856ad8305bef4bc147e8e31cbdf1b8e0249)\n",
            "Loading cached processed dataset at C:\\Users\\Ahmad\\.cache\\huggingface\\datasets\\hatexplain\\plain_text\\1.0.0\\df474d8d8667d89ef30649bf66e9c856ad8305bef4bc147e8e31cbdf1b8e0249\\cache-d5dd4819f594bfe0.arrow\n",
            "Loading cached processed dataset at C:\\Users\\Ahmad\\.cache\\huggingface\\datasets\\hatexplain\\plain_text\\1.0.0\\df474d8d8667d89ef30649bf66e9c856ad8305bef4bc147e8e31cbdf1b8e0249\\cache-512fd663cee324b1.arrow\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1, 1, 1]\n",
            "1\n"
          ]
        }
      ],
      "source": [
        "dataset = load_dataset(\"hatexplain\", split=\"validation\")\n",
        "\n",
        "class_names = [\"hate speech\", \"normal\", \"offensive\"]\n",
        "\n",
        "dataset = dataset.map(lambda e: {'sent_tokens' : ' '.join(e['post_tokens'])})\n",
        "dataset = dataset.map(lambda e: {'label' : int(np.median(e['annotators']['label']))})\n",
        "\n",
        "dataset_tensor = dataset.with_format(type='tensorflow', columns=['sent_tokens'])\n",
        "\n",
        "input_tensor = []\n",
        "for i in range(0,len(dataset_tensor)):\n",
        "    input_tensor += [dataset_tensor[i]['sent_tokens']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKnLPSEmtp9i"
      },
      "source": [
        "## Loading the BERT model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "tXxYpK8ixL34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject'>\n"
          ]
        }
      ],
      "source": [
        "model_name = 'hate_speech_bert_L-12_H-768_A-12'\n",
        "saved_model_path = './{}'.format(model_name.replace('/', '_'))\n",
        "\n",
        "\n",
        "reloaded_model = hub.load(saved_model_path)\n",
        "print(type(reloaded_model))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'reloaded_model' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_55952/1080191590.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m         \u001b[0mo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreloaded_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_tensor\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sent_tokens'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'reloaded_model' is not defined"
          ]
        }
      ],
      "source": [
        "outputs = []\n",
        "with tf.device(device):\n",
        "    for x in range(0,len(dataset_tensor), batch_size):\n",
        "        o = reloaded_model(dataset_tensor['sent_tokens'][x:x+batch_size])\n",
        "        outputs += (dataset['label'][x:x+batch_size], o) \n",
        "print(outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "classify_text_with_bert.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
