{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HateXplain\n",
    "\n",
    "https://huggingface.co/Hate-speech-CNERG/bert-base-uncased-hatexplain-rationale-two\n",
    "\n",
    "\n",
    "The model is used for classifying a text as Abusive (Hatespeech and Offensive) or Normal. The model is trained using data from Gab and Twitter and Human Rationales were included as part of the training data to boost the performance. The model also has a rationale predictor head that can predict the rationales given an abusive sentence.\n",
    "\n",
    "The dataset and models are available here: https://github.com/punyajoy/HateXplain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, BatchEncoding\n",
    "### from models.py\n",
    "from models import *\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "seed = 42\n",
    "sample_size = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Hate-speech-CNERG/bert-base-uncased-hatexplain-rationale-two\")\n",
    "model = Model_Rational_Label.from_pretrained(\"Hate-speech-CNERG/bert-base-uncased-hatexplain-rationale-two\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading in train and validaiton dataset\n",
    "hate_data = pd.read_csv(\"datasets\\hate speech tweets\\labeled_data.csv\").drop(columns=[\"Unnamed: 0\", \"count\", \"hate_speech\", \"offensive_language\", \"neither\"]).rename(columns={'class' : 'label', \"tweet\":\"text\"})\n",
    "\n",
    "# for human readability\n",
    "class_names = [\"hate speech\", \"offensive language\", \"neither\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379\n",
    "\n",
    "def batch_encode(tokenizer, texts, batch_size=256, max_length=MAX_LENGTH):\n",
    "    \"\"\"\"\"\"\"\"\"\n",
    "    A function that encodes a batch of texts and returns the texts'\n",
    "    corresponding encodings and attention masks that are ready to be fed \n",
    "    into a pre-trained transformer model.\n",
    "    \n",
    "    Input:\n",
    "        - tokenizer:   Tokenizer object from the PreTrainedTokenizer Class\n",
    "        - texts:       List of strings where each string represents a text\n",
    "        - batch_size:  Integer controlling number of texts in a batch\n",
    "        - max_length:  Integer controlling max number of words to tokenize in a given text\n",
    "    Output:\n",
    "        - input_ids:       sequence of texts encoded as a tf.Tensor object\n",
    "        - attention_mask:  the texts' attention mask encoded as a tf.Tensor object\n",
    "    \"\"\"\"\"\"\"\"\"\n",
    "    \n",
    "    input_ids = []\n",
    "    attention_mask = []\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        inputs = tokenizer.batch_encode_plus(batch,\n",
    "                                             padding='longest', #implements dynamic padding\n",
    "                                             return_attention_mask=True,\n",
    "                                             return_token_type_ids=False\n",
    "                                             )\n",
    "        input_ids.extend(inputs['input_ids'])\n",
    "        attention_mask.extend(inputs['attention_mask'])\n",
    "    \n",
    "    \n",
    "    return tf.convert_to_tensor(input_ids), tf.convert_to_tensor(attention_mask)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = hate_data.text.sample(sample_size).values.tolist()\n",
    "\n",
    "\n",
    "batchencoder = (tokenizer(data, return_tensors=\"pt\", padding=True))\n",
    "\n",
    "# print(batchencoder['input_ids'])\n",
    "#                        padding='longest', #implements dynamic padding\n",
    "#                        return_attention_mask=True,\n",
    "#                        return_token_type_ids=False).convert_to_tensors(\"tf\")\n",
    "\n",
    "# input_ids = batchencoder.convert_to_tensors(\"tf\")\n",
    "# max_length = np.max(list(map(len, data)))\n",
    "# print(max_length)\n",
    "# input_ids, attention_mask = batch_encode(tokenizer=tokenizer, texts=data, max_length=max_length)\n",
    "# print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_logits, _ = model(input_ids=batchencoder['input_ids'],attention_mask=batchencoder['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicitons = torch.argmax(prediction_logits, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: @BitchsLoveMosa do these bitches really love Mosa though #MosaGang http://t.co/0g1J8Bibpl : label: offensive language\n",
      "input: That depression shit trash. : label: hate speech\n",
      "input: RT @_____redd: killing these bitches go body for body &#128530;&#128524;&#127911; : label: offensive language\n",
      "input: \"@TheoMaxximus: Chrus still supahot tho\" *walks up to teacher with test, drops test in that bitch fohead* http://t.co/nb5VgdssgI : label: hate speech\n",
      "input: @aasthaxoxo oh shut your mouth i was talkin about the morning you fag : label: offensive language\n",
      "input: Dem hoes be peeping out my dougie &#128131; : label: offensive language\n",
      "input: @UberFacts studies show that you're a faggot : label: offensive language\n",
      "input: I don't want sex bitch I only want ya neck bitch lol : label: offensive language\n",
      "input: RT @_RealThickJazz: \"@1inkkofrosess: Makem eat my ass since eating pussy ain't major nomore &#128175;&#128553;&#128514;&#9996;&#65039;&#128584;&#128129;&#128079;&#128522;\" &#128514;&#128514; : label: hate speech\n",
      "input: RT @Opatz1: Fuck michelle obama. That monkey doesnt belong in the white house : label: offensive language\n",
      "input: Lmao this bitch. : label: offensive language\n",
      "input: RT @BestSagittarius: #Sagittarius can be compassionate towards other people when they want to be, especially towards the ones they care abo&#8230; : label: hate speech\n",
      "input: Vine is filled with pussy lol smh : label: hate speech\n",
      "input: RT @jakeburnsmfc: @staycoolwheels @BloggerBoxing because hillbillies like sports to... : label: hate speech\n",
      "input: @adamjferraro I'm actually with my boys right now. Your dorky ass is with your mom &amp; dad watching Streit make you his bitch : label: hate speech\n",
      "input: RT @NoHoesNextDoor: Mimi so proud to be a hoe : label: hate speech\n",
      "input: I swear I needa go on n go back to the old me because bitches think its a game apparently. Let's be real ! : label: offensive language\n",
      "input: Speak to my face you fake bitches. : label: offensive language\n",
      "input: You a dike cause yo man a pussy. : label: offensive language\n",
      "input: God you're a faggot if you dont think superman is the best superhero : label: offensive language\n",
      "input: #BestSongToHaveSexTo bitches love sosa #bangbang : label: hate speech\n",
      "input: RT @ChristineKKTV: Great weather to hit up the slopes this weekend... @Arapahoe_Basin &amp; @LovelandSkiArea open Saturday! #COwx #skiseason ht&#8230; : label: hate speech\n",
      "input: Tracy Morgans car wreck and the press was covered by Zappa in the 60's...\"Some joker with a brownie and ye'll see it all complete\" : label: hate speech\n",
      "input: sheryl crow being the bae : label: hate speech\n",
      "input: I ont like no dramatic bitch so I just separate myself n shit : label: hate speech\n",
      "input: how many fingers does a bitch gotta break so mother fuckers will stop taking my god damn lighters : label: offensive language\n",
      "input: @tina16_ that bitch gonna get a rug burn : label: offensive language\n",
      "input: RT @_LastQueenCyn_: Niggas be in that pussy like http://t.co/DDJSB5itcD : label: hate speech\n",
      "input: I curved yo bitch. Your welcome. : label: offensive language\n",
      "input: RT @NevaWhoDat: wet pussy is the best pussy &#128588;&#128527;&#128540;&#128069;&#128166;&#128572;&#128175; : label: hate speech\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_my_examples(inputs, results):\n",
    "  result_for_printing = \\\n",
    "    [f'input: {inputs[i]} : label: {class_names[results[i]]}'\n",
    "                         for i in range(len(inputs))]\n",
    "  print(*result_for_printing, sep='\\n')\n",
    "  print()\n",
    "\n",
    "print_my_examples(data, predicitons.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "af2dc65967692050cab8e66feb265a08b4036dabf5bf167c8e1e4da3fe6ea63e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
