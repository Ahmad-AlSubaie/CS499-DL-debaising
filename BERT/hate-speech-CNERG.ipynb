{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HateXplain\n",
    "\n",
    "https://huggingface.co/Hate-speech-CNERG/bert-base-uncased-hatexplain-rationale-two\n",
    "\n",
    "\n",
    "The model is used for classifying a text as Abusive (Hatespeech and Offensive) or Normal. The model is trained using data from Gab and Twitter and Human Rationales were included as part of the training data to boost the performance. The model also has a rationale predictor head that can predict the rationales given an abusive sentence.\n",
    "\n",
    "The dataset and models are available here: https://github.com/punyajoy/HateXplain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, BatchEncoding\n",
    "### from models.py\n",
    "from models import *\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from official.nlp import optimization  # to create AdamW optimizer\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.python.ops.numpy_ops import np_config\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "seed = 42\n",
    "batch_size = 10\n",
    "test_size = 30\n",
    "MAX_LENGTH = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Hate-speech-CNERG/bert-base-uncased-hatexplain-rationale-two\")\n",
    "model = Model_Rational_Label.from_pretrained(\"Hate-speech-CNERG/bert-base-uncased-hatexplain-rationale-two\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading in testing data set\n",
    "customer_data = pd.read_csv(\"datasets\\\\Customer support tweets\\\\twcs.csv\").drop(columns=[\"tweet_id\", \"author_id\", \"inbound\", \"created_at\", \"response_tweet_id\",\"in_response_to_tweet_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379\n",
    "\n",
    "def batch_encode(tokenizer, texts, batch_size=256, max_length=MAX_LENGTH):\n",
    "    \"\"\"\"\"\"\"\"\"\n",
    "    A function that encodes a batch of texts and returns the texts'\n",
    "    corresponding encodings and attention masks that are ready to be fed \n",
    "    into a pre-trained transformer model.\n",
    "    \n",
    "    Input:\n",
    "        - tokenizer:   Tokenizer object from the PreTrainedTokenizer Class\n",
    "        - texts:       List of strings where each string represents a text\n",
    "        - batch_size:  Integer controlling number of texts in a batch\n",
    "        - max_length:  Integer controlling max number of words to tokenize in a given text\n",
    "    Output:\n",
    "        - input_ids:       sequence of texts encoded as a tf.Tensor object\n",
    "        - attention_mask:  the texts' attention mask encoded as a tf.Tensor object\n",
    "    \"\"\"\"\"\"\"\"\"\n",
    "    \n",
    "    input_ids = []\n",
    "    attention_mask = []\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        inputs = tokenizer.batch_encode_plus(batch,\n",
    "                                             padding='longest', #implements dynamic padding\n",
    "                                             return_attention_mask=True,\n",
    "                                             return_token_type_ids=False\n",
    "                                             )\n",
    "        input_ids.extend(inputs['input_ids'])\n",
    "        attention_mask.extend(inputs['attention_mask'])\n",
    "    \n",
    "    \n",
    "    return tf.convert_to_tensor(input_ids), tf.convert_to_tensor(attention_mask)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  1030, 23090,  2581, 21472, 22091,  2860,  4283,  1010,  4074,\n",
      "          1012,  1045,  2572,  2383,  1037,  2204, 15060,  1012,  1011, 10225,\n",
      "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  1030, 16327, 21619,  2549,  2057,  1005,  2128,  3374,  2000,\n",
      "          2963,  2023,  1010,  4575,  1012,  2057,  2079,  3246,  2115,  3462,\n",
      "          2067,  2003,  2172,  2062, 22249,  2174,  1012,  4283,  2005,  1996,\n",
      "         12247,   999,  1034,  8840,  9215,   102,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  1030, 24559, 22407,  2629,  4931,   999,  2057,  1521,  2128,\n",
      "          3374,  2000,  2963,  2008,  1012,  4638,  2041, 16770,  1024,  1013,\n",
      "          1013,  1056,  1012,  2522,  1013,  1053,  7295,  2509,  6559,  2620,\n",
      "          2100,  2475,  2078,  2005,  2054,  2000,  2079,  2279,  1013,  1050,\n",
      "          4160,   102,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  1030,  4724, 14526, 22932,  2057,  3749,  2490,  3081, 10474,\n",
      "          1999,  2394,  1012,  3967,  2149,  2005,  2393,  1999,  2115,  6871,\n",
      "          2653,  2182,  1024, 16770,  1024,  1013,  1013,  1056,  1012,  2522,\n",
      "          1013, 21307, 28008,  2509,  2615, 24798,  2361,  3501,   102,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  1030,  5354, 22203, 23833,  2031,  2017, 11925,  1996,  6432,\n",
      "          3495,  1029,  2031,  2017,  2170,  2630,  9001,  3495,  1030,  6584,\n",
      "          2581,  1011,  4466,  2575,  1011,  5354, 21057,  1029,  1034, 25260,\n",
      "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  1030,  4583,  2683,  2683,  2575,  2487,  1045,  1005,  1049,\n",
      "          5580,  2115,  3462,  2001, 22249,  1010,  2508,  1012,  2009,  2515,\n",
      "          2191,  1037,  4489,  2043,  2017,  2031,  2307,  3626,  2559,  2044,\n",
      "          2017,  1010,  2987,  1005,  1056,  2009,  1029,  1034, 11409,  5104,\n",
      "           102,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  1030,  2250, 15396,  6342,  9397, 11589,  4067,  2017,  2005,\n",
      "          9805,  2099,  4248,  3433,   999,  1024,  1007,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  1030,  3962,  8757, 16302,  2015,  7632,  1010,  4364,   999,\n",
      "          1045,  2001,  6603,  2065,  2045,  2003,  1037,  7163,  1011,  2447,\n",
      "          2006,  3962,  8757,  1005,  1055, 15363, 10439,  2006,  3645,  2184,\n",
      "          1998,  1045,  1005,  1049,  2074,  2000,  6397,  2000,  2424,  2009,\n",
      "          1029, 16215,  2595,   999,   102,     0,     0,     0,     0],\n",
      "        [  101,  1030,  9733, 16001,  2361,  1045,  2031,  1037,  2210,  4792,\n",
      "          1012,  3967,  1030, 10630, 27531,  2487,  1012,  2064,  2017,  2393,\n",
      "          2033,  1029,  2488,  3081, 10373,  2030,  7511,  5653,  1029,  2002,\n",
      "          2003,  2200,  5697,  1998,  1045,  2572,  2053,  2028,  1012,  2129,\n",
      "          2079,  1045,  2079,  2009,  1029,  1024,  1011,  1006,   102],\n",
      "        [  101,  1030,  7160,  2071,  1045,  3531,  2131,  2070,  2393,  1030,\n",
      "          9361,  1999,  2005,  2026,  2022,  2954,  1029,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0]])\n"
     ]
    }
   ],
   "source": [
    "data = customer_data.text.sample(test_size).values.tolist()\n",
    "input_ids = []\n",
    "attention_mask = []\n",
    "\n",
    "\n",
    "batchencoder = (tokenizer(data[0:10], return_tensors=\"pt\", padding=True))\n",
    "\n",
    "print(batchencoder['input_ids'])\n",
    "#                        padding='longest', #implements dynamic padding\n",
    "#                        return_attention_mask=True,\n",
    "#                        return_token_type_ids=False).convert_to_tensors(\"tf\")\n",
    "\n",
    "# input_ids = batchencoder.convert_to_tensors(\"tf\")\n",
    "# max_length = np.max(list(map(len, data)))\n",
    "# print(max_length)\n",
    "# input_ids, attention_mask = batch_encode(tokenizer=tokenizer, texts=data, max_length=max_length)\n",
    "# print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_logits, _ = model(input_ids=batchencoder['input_ids'],attention_mask=batchencoder['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(prediction_logits, dim=1)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "af2dc65967692050cab8e66feb265a08b4036dabf5bf167c8e1e4da3fe6ea63e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
