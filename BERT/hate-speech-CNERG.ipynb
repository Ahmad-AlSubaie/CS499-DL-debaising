{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HateXplain\n",
    "\n",
    "https://huggingface.co/Hate-speech-CNERG/bert-base-uncased-hatexplain-rationale-two\n",
    "\n",
    "\n",
    "The model is used for classifying a text as Abusive (Hatespeech and Offensive) or Normal. The model is trained using data from Gab and Twitter and Human Rationales were included as part of the training data to boost the performance. The model also has a rationale predictor head that can predict the rationales given an abusive sentence.\n",
    "\n",
    "The dataset and models are available here: https://github.com/punyajoy/HateXplain\n",
    "\n",
    "\n",
    "Binny Mathew, Punyajoy Saha, Seid Muhie Yimam, Chris Biemann, Pawan Goyal, and Animesh Mukherjee \"[HateXplain: A Benchmark Dataset for Explainable Hate Speech Detection)\". Accepted at AAAI 2021.\n",
    "\n",
    "\n",
    "@article{mathew2020hatexplain,\n",
    "  title={HateXplain: A Benchmark Dataset for Explainable Hate Speech Detection},\n",
    "  author={Mathew, Binny and Saha, Punyajoy and Yimam, Seid Muhie and Biemann, Chris and Goyal, Pawan and Mukherjee, Animesh},\n",
    "  journal={arXiv preprint arXiv:2012.10289},\n",
    "  year={2020}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "### from models.py\n",
    "from models import *\n",
    "import os\n",
    "import shutil\n",
    "from datetime import date\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "seed = 42\n",
    "batch_size = 1\n",
    "MODEL_NAME = \"HateXplain\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading model \n",
    "It is in the cuda device if is avalible.\n",
    "\n",
    "Classifies sentences into \"hate speech\", \"normal\", \"offensive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Hate-speech-CNERG/bert-base-uncased-hatexplain\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"Hate-speech-CNERG/bert-base-uncased-hatexplain\").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading dataset\n",
    "This is the hatexplain dataset, with sentence tokens, each annotators labels, reasoning, and hate targets. \n",
    "\n",
    "In this situaiton we are only using the sentence tokens and labels as a standarized benchmark dataset to comapre across models\n",
    "\n",
    "This model was trianed on this dataset, so the benchmarking quality in this context is not as assured as we'd like\n",
    "\n",
    "Data is tokenized and batched here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"hatexplain\", split=\"validation\")\n",
    "\n",
    "class_names = [\"hate speech\", \"normal\", \"offensive\"]\n",
    "\n",
    "data_tokens = tokenizer(dataset['post_tokens'], padding=True,is_split_into_words=True, return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "input_tensor = []\n",
    "attention_mask_tensor = []\n",
    "for i in range(0,len(data_tokens.input_ids), batch_size):\n",
    "    input_tensor += [data_tokens.input_ids[i:i+batch_size]]\n",
    "    attention_mask_tensor += [data_tokens.attention_mask[i:i+batch_size]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    "Data is loaded into cuda/device, the model then runs a prediction, then results are unpacked and runs softmax(ed).\n",
    "\n",
    "We do this becuse all the data and model combined are >8GB in memory, so we need to load and unload data continuously\n",
    "\n",
    "also the softmax is done in cuda/device/pytorch, its convinient to do here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicitons = []\n",
    "for i in range(len(input_tensor)):\n",
    "    with torch.no_grad():\n",
    "        inp = input_tensor[i].to(device)\n",
    "        attention = attention_mask_tensor[i].to(device)\n",
    "        prediction_logits = model.forward(input_ids=inp,attention_mask=attention).logits\n",
    "\n",
    "        for x in prediction_logits: # for each batch per output \n",
    "            predicitons.append(torch.argmax(x, dim=0).item())\n",
    "\n",
    "        inp.detach()\n",
    "        attention.detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pack predictions and gold/true lables together\n",
    "\n",
    "I have not 100% sure if the true gold labels match with thier coresponding predicitons. I think they do.\n",
    "\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_with_labels = []\n",
    "for i in range(len(predicitons)):\n",
    "    predictions_with_labels.append((predicitons[i], int(np.median(dataset[i]['annotators']['label']))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions_with_labels[0])\n",
    "predction_data = pd.DataFrame(predictions_with_labels, columns=[\"prediciton\", \"annotators_labels\"])\n",
    "predction_data.to_csv(path_or_buf=f\"outputs\\\\{MODEL_NAME}_{date.today()}.csv\", index=False)\n",
    "print(predction_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"dataset size: {len(predictions_with_labels)}\")\n",
    "for i in range(len(predictions_with_labels)):\n",
    "    print(\n",
    "        f\"data: {' '.join(dataset[i]['post_tokens'])} \" +\n",
    "        f\"\\n\\tpredictiton: {predictions_with_labels[i][0]} {class_names[predictions_with_labels[i][0]]} \"+\n",
    "        f\"\\n\\trecorded label: {predictions_with_labels[i][1]}\"+\n",
    "        f\"\\n\\ttrue lable: {dataset[i]['annotators']['label']} {class_names[int(np.median(dataset[i]['annotators']['label']))]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "af2dc65967692050cab8e66feb265a08b4036dabf5bf167c8e1e4da3fe6ea63e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
